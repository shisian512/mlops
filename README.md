-----

# ğŸš€ MLOps End-to-End Regression Pipeline

Production-grade MLOps pipeline for regression tasks. Covers data prep, training, deployment, monitoring, and CI/CDâ€”all containerized and cloud-ready.

-----

## Architecture Diagram

Below is the autogenerated system architecture diagram:

-----

## ğŸ“ Project Structure

```text
(WIP)
```

-----

## ğŸ“ˆ CI/CD & MLOps Workflow

Our end-to-end pipeline is triggered by a code push to the Git repository and orchestrated by Airflow. It consists of the following key stages:

**CI Pipeline**
This upstream pipeline is handled by GitHub Actions and focuses on code quality and containerization.

  - An engineer pushes code to the repository.
  - Linting (Flake8, Black) and unit tests (pytest) are run.
  - Docker images are built and pushed to Amazon ECR.

**ETL Pipeline (Airflow)**
This DAG processes and versions the data, ensuring a consistent and reliable feature set for training.

  - Data Ingestion: A Spark job pulls raw data from Amazon S3.
  - Data Preprocessing: Another Spark job cleans and transforms the raw data.
  - Data Validation: A validation task ensures the data quality is high before proceeding.
  - Data Versioning: The cleaned data is tracked and versioned using DVC.
  - Feature Store: The final feature set is loaded into a DynamoDB feature store.

**Training Pipeline (Airflow)**
This DAG orchestrates the entire model training and promotion lifecycle.

  - Data Preparation: The DAG uses DVC to fetch a specific, version-controlled dataset from S3 and prepares it for training.
  - Model Training: A SageMaker Training Job is triggered to train a new model. All parameters and metrics are logged to our MLflow server.
  - Post-Training Analysis: After training, a dedicated task evaluates the model's performance on a test set and generates interpretability reports using SHAP. The results are logged as artifacts in MLflow.
  - Model Promotion: A Python script compares the new model (challenger) against the current production model (champion). If the challenger performs better, it is automatically assigned the staging alias in the MLflow Model Registry.
  - Deployment Trigger: The final task modifies the Kubernetes deployment configuration file, commits the change to Git, and pushes it. This triggers a GitOps workflow (e.g., via Argo CD) to begin the deployment of the new model.

-----

## ğŸ‘¨â€ğŸ’» For Developers: How to Contribute

We welcome and encourage contributions from the community\! To get started, please follow this process.

### **GitHub Issues**

We use **GitHub Issues** to track bugs, enhancements, and upcoming work. Before you start working on something, please check the existing issues to see if a similar task has already been created.

  * **Reporting Bugs:** If you find a bug, please open a new issue with the `bug` label.
  * **Suggesting Enhancements:** If you have an idea for a new feature, please open an issue with the `enhancement` label.

### **Pull Request (PR) Workflow**

To contribute code, please follow these steps:

1.  **Fork** this repository to your own GitHub account.
2.  **Clone** your forked repository to your local machine.
3.  Create a new **branch** from `main` with a descriptive name (e.g., `feat/add-new-model` or `fix/etl-pipeline`).
4.  Make your changes and ensure your code follows the project's standards (linting with Flake8, formatting with Black).
5.  Run all existing tests with `pytest` to ensure your changes haven't introduced any regressions.
6.  **Commit** your changes with a clear and concise message.
7.  **Push** your changes to your fork.
8.  Open a **Pull Request** from your branch to the `main` branch of this repository.

-----

## â–¶ï¸ Quickstart

1.  **Prerequisites**: Docker, Docker Compose, **Poetry**

2.  **Install Python dependencies:**

<!-- end list -->

```bash
poetry install
```

3.  **Start all services**:

<!-- end list -->

```bash
docker-compose up --build
```

4.  **Access Services**:

| Service Â  Â  Â | URL Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â |
| ------------ | -------------------------------------------------------- |
| MLflow Â  Â  Â  | [http://localhost:5000](https://www.google.com/search?q=http://localhost:5000) Â  Â  Â  Â  Â  |
| FastAPI Â  Â  Â | [http://localhost:8000/docs](https://www.google.com/search?q=http://localhost:8000/docs) |
| Streamlit Â  Â | [http://localhost:8501](https://www.google.com/search?q=http://localhost:8501) Â  Â  Â  Â  Â  |
| Grafana Â  Â  Â | [http://localhost:3000](https://www.google.com/search?q=http://localhost:3000) Â  Â  Â  Â  Â  |
| Prometheus Â  | [http://localhost:9090](https://www.google.com/search?q=http://localhost:9090) Â  Â  Â  Â  Â  |
| cAdvisor Â  Â  | [http://localhost:8080](https://www.google.com/search?q=http://localhost:8080) Â  Â  Â  Â  Â  |
| Alertmanager | [http://localhost:9093](https://www.google.com/search?q=http://localhost:9093) Â  Â  Â  Â  Â  |

-----

## ğŸ—ºï¸ Roadmap

  * ğŸ” Automated retraining (on drift)
  * ğŸ” Auth, secrets, RBAC for API/UI
  * â˜ï¸ Terraform + Helm for cloud rollout
  * ğŸ§  Feature Store integration
  * ğŸš¦ Canary/Blue-Green deployment workflows

-----

## ğŸ™Œ Credits

Monitoring stack based on [dockprom](https://github.com/stefanprodan/dockprom) by Stefan Prodan (MIT License).

-----

## ğŸ‘¤ Author

Personal project by [@shisian512](https://github.com/shisian512).
Fork, scale, and use it for your own MLOps workflow.