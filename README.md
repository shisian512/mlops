# ğŸš€ MLOps End-to-End Regression Pipeline

Production-grade MLOps pipeline for regression tasks. Covers data prep, training, deployment, monitoring, and CI/CDâ€”all containerized and cloud-ready.

---

## Architecture Diagram

Below is the autogenerated system architecture diagram:

![MLOps Pipeline architecture](diagram/mlops_e2e_pipeline.png)

---

## ğŸ“ Project Structure

```text
(WIP)

---

## ğŸ“ˆ CI/CD & MLOps Workflow
Our end-to-end pipeline is triggered by a code push to the Git repository and orchestrated by Airflow. It consists of the following key stages:

**CI Pipeline**
This upstream pipeline is handled by GitHub Actions and focuses on code quality and containerization.

An engineer pushes code to the repository.
Linting (Flake8, Black) and unit tests (pytest) are run.
Docker images are built and pushed to Amazon ECR.

**ETL Pipeline (Airflow)**
This DAG processes and versions the data, ensuring a consistent and reliable feature set for training.

Data Ingestion: A Spark job pulls raw data from Amazon S3.
Data Preprocessing: Another Spark job cleans and transforms the raw data.
Data Validation: A validation task ensures the data quality is high before proceeding.
Data Versioning: The cleaned data is tracked and versioned using DVC.
Feature Store: The final feature set is loaded into a DynamoDB feature store.

**Training Pipeline (Airflow)**
This DAG orchestrates the entire model training and promotion lifecycle.

Data Preparation: The DAG uses DVC to fetch a specific, version-controlled dataset from S3 and prepares it for training.
Model Training: A SageMaker Training Job is triggered to train a new model. All parameters and metrics are logged to our MLflow server.
Post-Training Analysis: After training, a dedicated task evaluates the model's performance on a test set and generates interpretability reports using SHAP. The results are logged as artifacts in MLflow.
Model Promotion: A Python script compares the new model (challenger) against the current production model (champion). If the challenger performs better, it is automatically assigned the staging alias in the MLflow Model Registry.
Deployment Trigger: The final task modifies the Kubernetes deployment configuration file, commits the change to Git, and pushes it. This triggers a GitOps workflow (e.g., via Argo CD) to begin the deployment of the new model.

---
## â–¶ï¸ Quickstart

1. **Prerequisites**: Docker, Docker Compose, **Poetry**

2. **Install Python dependencies:**

```bash
poetry install
```

3. **Start all services**:

```bash
docker-compose up --build
```

4. **Access Services**:

| Service      | URL                                                      |
| ------------ | -------------------------------------------------------- |
| MLflow       | [http://localhost:5000](http://localhost:5000)           |
| FastAPI      | [http://localhost:8000/docs](http://localhost:8000/docs) |
| Streamlit    | [http://localhost:8501](http://localhost:8501)           |
| Grafana      | [http://localhost:3000](http://localhost:3000)           |
| Prometheus   | [http://localhost:9090](http://localhost:9090)           |
| cAdvisor     | [http://localhost:8080](http://localhost:8080)           |
| Alertmanager | [http://localhost:9093](http://localhost:9093)           |

---

## ğŸ—ºï¸ Roadmap

* ğŸ” Automated retraining (on drift)
* ğŸ” Auth, secrets, RBAC for API/UI
* â˜ï¸ Terraform + Helm for cloud rollout
* ğŸ§  Feature Store integration
* ğŸš¦ Canary/Blue-Green deployment workflows

---

## ğŸ™Œ Credits

Monitoring stack based on [dockprom](https://github.com/stefanprodan/dockprom) by Stefan Prodan (MIT License).

---

## ğŸ‘¤ Author

Personal project by [@shisian512](https://github.com/shisian512).
Fork, scale, and use it for your own MLOps workflow.
